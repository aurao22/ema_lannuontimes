{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as a dll could not be loaded. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details.\n",
      "<a href='https://aka.ms/kernelFailuresDllLoad'>Learn more</a>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from news_paper_dao import NewsPaperDao\n",
    "from IPython.core.display import HTML\n",
    "from os import getcwd, path\n",
    "from news_paper_nlp_pre_processing import *\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupère le répertoire du programme\n",
    "curent_path = getcwd()+ \"\\\\\"\n",
    "if \"ema_lannuontimes\" not in curent_path:\n",
    "    curent_path += \"PROJETS\\\\ema_lannuontimes\\\\\"\n",
    "print(curent_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: Blue; padding: 15px;\" >\n",
    "\n",
    "## 1.Exploration des données: \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "\n",
    "dao = NewsPaperDao(nom_bdd=curent_path+\"em_bdd.db\")\n",
    "assert dao.test_connexion()\n",
    "\n",
    "df_articles = dao.get_articles(verbose=0)\n",
    "print(df_articles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_by_journal = {\"Le Trégor\":\"green\",\"ActuGaming\":\"orange\", \"Elle\":\"pink\", \"30 M. d\\\\'amis\":\"blue\"}\n",
    "journaux = df_articles[\"journal\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_articles.shape)\n",
    "print(df_articles.columns)\n",
    "display(HTML(df_articles.head().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axe = color_graph_background(1, 1)\n",
    "\n",
    "bins = len(journaux)\n",
    "for journal in journaux:\n",
    "    df_articles[df_articles[\"journal\"]==journal].journal.hist(ax=axe, color=color_by_journal.get(journal, \"gray\"), bins=bins)\n",
    "\n",
    "figure.set_size_inches(15, 5, forward=True)\n",
    "axe.set_ylabel(\"Nombre d'articles\")\n",
    "plt.title(\"Nombre d'articles par journal\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = df_word_tokenize(df_articles, text_col_name='texte', token_col_name=\"mots_origine\", verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles[\"nb_mots_origine\"] = df_articles['texte'].apply(lambda x: word_count_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = df_articles.groupby([\"journal\"], as_index=True).agg({'nb_mots_origine':['mean']})\n",
    "group = group.reset_index()\n",
    "group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tuple = [(i, i+299) for i in range (600, 3300, 300)]\n",
    "list_tuple.insert(0, (200, 299))\n",
    "list_tuple.insert(0, (100, 199))\n",
    "list_tuple.insert(0, (0, 99))\n",
    "list_tuple.append((3300, 100000))\n",
    "bins = pd.IntervalIndex.from_tuples(list_tuple)\n",
    "df_articles[\"tranche_nb_origin\"] = pd.cut(df_articles['nb_mots_origine'], bins)\n",
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_nb_mots_art = df_articles.groupby([\"journal\", \"tranche_nb_origin\" ])[\"titre\"].count().unstack(\"journal\").fillna(0)\n",
    "group_nb_mots_art.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axe = color_graph_background(1,1)\n",
    "\n",
    "# Affichage d'articles par journal\n",
    "group_nb_mots_art.plot(kind='bar', ax=axe)\n",
    "axe.set_ylabel(\"Nombre d'articles\")\n",
    "axe.grid(axis='y')\n",
    "\n",
    "figure.set_size_inches(16, 8, forward=True)\n",
    "figure.suptitle(\"Nombre d'articles par tranche de mots\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: Blue; padding: 15px;\" >\n",
    "\n",
    "## 3.NLP Preprocessing\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personnal_stop_word = [\"tout\", \"tous\", \"cette\", \"bien\", \"comme\", \"encore\", \"autre\", \"bien\", \"tres\", \"alors\", \"plus\", \"aussi\", \"si\", \"donc\", \"p\", \"h\", \"etre\"]\n",
    "if personnal_stop_word is None:\n",
    "    personnal_stop_word = stopwords.words(\"french\")\n",
    "else :\n",
    "    personnal_stop_word.extend(stopwords.words(\"french\"))\n",
    "len(personnal_stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_pre_process(input, sw=None, verbose=0):\n",
    "    res = []\n",
    "    res = tokenize(input)\n",
    "    res = remove_stopwords_func(res, sw=sw)\n",
    "    res = normalize_accented_chars(res)\n",
    "    res = remove_irr_char_func(res)        \n",
    "    res = remove_stopwords_func(res, sw=sw)\n",
    "        \n",
    "    res = list(filter(None, res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles[\"clean_words\"] = df_articles['texte'].apply(lambda x: nlp_pre_process(x, sw=personnal_stop_word))\n",
    "print(df_articles.columns)\n",
    "display(HTML(df_articles.head().to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: Green; padding: 7px;\" >\n",
    "\n",
    "### 3.1.Analyse nb mots\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles[\"nb_mots_clean\"] = df_articles['clean_words'].apply(lambda x: word_count_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles[\"freq_name\"] = df_articles[\"clean_words\"].apply(lambda x: nltk.FreqDist(x))\n",
    "df_articles[\"freq_unique_words\"] = df_articles[\"freq_name\"].apply(lambda x: len(x.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = df_articles.sort_values(by=[\"freq_unique_words\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axe = color_graph_background(1,1)\n",
    "\n",
    "df_articles.plot(kind='bar', x='titre', y=\"nb_mots_clean\", ax=axe, title='Nombre de mots par article')\n",
    "df_articles.plot(y=\"freq_unique_words\", x='titre', kind=\"bar\", ax=axe, color=\"red\")\n",
    "figure.set_size_inches(16, 8, forward=True)\n",
    "\n",
    "plt.ylabel(\"Nombre de mots par article\")\n",
    "plt.xlabel(\"\")\n",
    "plt.xticks([])\n",
    "# plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = color_graph_background(len(journaux),1)\n",
    "\n",
    "y_ticks = [i for i in range(0, 1200, 200)]\n",
    "\n",
    "i = 0\n",
    "for journal in journaux:\n",
    "    df_articles[df_articles[\"journal\"]==journal].plot(y=\"freq_unique_words\", x='titre', label=journal, kind=\"bar\", ax=axes[i], color=color_by_journal.get(journal, \"gray\"))\n",
    "    axes[i].set_yticks(y_ticks)\n",
    "    axes[i].set_xticks([])\n",
    "    i += 1\n",
    "\n",
    "figure.set_size_inches(16, 8, forward=True)\n",
    "plt.suptitle('Nombre de mots uniques par article par journal', fontsize=16)\n",
    "plt.xlabel(\"Articles\")\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: Green; padding: 7px;\" >\n",
    "\n",
    "### 3.2.Nuage de mots\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: indigo;\" >\n",
    "\n",
    "#### 3.2.1. SANS lemmatisation\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles[\"texte_clean\"] = [','.join(map(str, l)) for l in df_articles[\"clean_words\"]]\n",
    "df_articles[\"sentence_clean\"] = [' '.join(map(str, l)) for l in df_articles[\"clean_words\"]]\n",
    "df_articles[\"texte_clean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for journal in journaux:\n",
    "    word_cloud(journal, df_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: indigo;\" >\n",
    "\n",
    "#### 3.2.2. AVEC lemmatisation\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: Blue; padding: 15px;\" >\n",
    "\n",
    "## 4.Entrainement d'un modèle de classification\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions :\n",
    "- Est-ce qu'on met en place l'approche td-idf ?\n",
    "- Est-ce qu'on fait la lemmatisation ?\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: Green; padding: 7px;\" >\n",
    "\n",
    "### 4.0.Préparer les données\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: indigo;\" >\n",
    "\n",
    "#### 4.0.1. Encodage de la target\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = 'journal_code'\n",
    "transformer_news_paper = LabelEncoder()\n",
    "df_articles[target_name] = transformer_news_paper.fit_transform(df_articles[\"journal\"])\n",
    "#-- On positionne la colonne juste avant le nom du journal pour plus de lisibilité\n",
    "cols = list(df_articles.columns)\n",
    "cols.remove(target_name)\n",
    "idx = cols.index('journal')\n",
    "cols.insert(idx, target_name)\n",
    "df_articles = df_articles[cols]\n",
    "#--\n",
    "df_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: Green; padding: 7px;\" >\n",
    "\n",
    "### 4.1.Essaie avec TF-IDF\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: indigo;\" >\n",
    "\n",
    "#### 4.0.2. Calcul\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_full = df_articles.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_idf_big = TfidfVectorizer(analyzer=\"word\",token_pattern=get_regex_tokens(), stop_words=personnal_stop_word, ngram_range = (1,1), max_features=400)\n",
    "X = vectorizer_idf_big.fit_transform(idf_full['sentence_clean'])\n",
    "idf_big_df = pd.DataFrame(X.toarray(), index=idf_full.index, columns=vectorizer_idf_big.get_feature_names_out())\n",
    "# ajout du site web\n",
    "\n",
    "# Réorganisation des colonnes pour la lecture\n",
    "idf_full = pd.merge(idf_full, idf_big_df, left_index=True, right_index=True)\n",
    "idf_full = idf_full.drop([\"mots_origine\", \"url\", \"nb_mots_origine\",'auteur', 'tags', 'nb_mots_clean','freq_name','freq_unique_words'], axis=1)\n",
    "# --\n",
    "idf_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: indigo;\" >\n",
    "\n",
    "#### 4.0.3. Préparer le test et train\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = get_numeric_columns_names(idf_full, verbose=verbose)\n",
    "numeric_cols.remove(target_name)\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On prend uniquement les colonnes qui nous intéresse (identifiée précédemment)\n",
    "X_train_idf, X_test_idf, y_train_idf, y_test_idf = train_test_split(idf_full[numeric_cols], idf_full[target_name], test_size=0.2, random_state=random_state)\n",
    "print(f\" Train : {X_train_idf.shape} et {y_train_idf.shape} --- Test : {X_test_idf.shape} et {y_test_idf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: indigo;\" >\n",
    "\n",
    "#### 4.0.4. Prédiction\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB,BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 0\n",
    "\n",
    "#Create classifier\n",
    "model_list = {\n",
    "    \"LogisticR\":LogisticRegression(random_state=random_state, verbose=verbose),\n",
    "    \"SVC\":svm.SVC(random_state=random_state, verbose=verbose), # SCORE de 0.643333\n",
    "    \"KNN\":KNeighborsClassifier(n_neighbors=3), # SCRORE de 0.558333\n",
    "    \"LinearSVC\":LinearSVC(random_state=random_state, verbose=verbose),\n",
    "    \"naiveGaussianNB\":GaussianNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"ComplementNB\": ComplementNB(),\n",
    "    \"BernoulliNB\":BernoulliNB()\n",
    "}\n",
    "\n",
    "# Train the model using the training sets\n",
    "model_dic_idf, scores_idf = fit_and_test_models(model_list, X_train=X_train_idf, Y_train=y_train_idf, X_test=X_test_idf, Y_test=y_test_idf, verbose=verbose)\n",
    "\n",
    "score_all_class_df = pd.DataFrame(scores_idf).set_index(\"Modeli\")\n",
    "score_all_class_df.round(decimals=3)\n",
    "score_all_class_df = score_all_class_df.sort_values(by=\"R2\", ascending=False)\n",
    "score_all_class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params_BernoulliNB = { 'alpha' : [0,1],\n",
    "                            'binarize' : [0.0, 0.5,1],\n",
    "                            'fit_prior' : [True,False]\n",
    "                            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation = 3\n",
    "grid_model_list = {\n",
    "    \"Grid_BernoulliNB\":GridSearchCV(estimator=BernoulliNB(), param_grid=grid_params_BernoulliNB, cv=cross_validation, verbose=verbose),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the training sets\n",
    "model_dic_idf_grid, scores_idf_grid = fit_and_test_models(grid_model_list, X_train=X_train_idf, Y_train=y_train_idf, X_test=X_test_idf, Y_test=y_test_idf, scores=scores_idf, verbose=verbose)\n",
    "\n",
    "score_grid_idf = pd.DataFrame(scores_idf_grid).set_index(\"Modeli\")\n",
    "score_grid_idf.round(decimals=3)\n",
    "score_grid_idf = score_grid_idf.sort_values(by=\"R2\", ascending=False)\n",
    "score_grid_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: Green; padding: 7px;\" >\n",
    "\n",
    "### 4.1.Essaie avec W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import downloader as g_dwnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dwnl.info()[\"models\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = g_dwnl.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: indigo;\" >\n",
    "\n",
    "#### 4.0.2. Calcul\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: indigo;\" >\n",
    "\n",
    "#### 4.0.3. Préparer le test et train\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: Green; padding: 7px;\" >\n",
    "\n",
    "### 4.1.Test de plusieurs modèles\n",
    "</div>\n",
    "\n",
    "*   Entrainer un modèle de classification\n",
    "*   Afficher la matrice de confusion\n",
    "*   Calculer l'accuracy, la précision et le recall\n",
    "*   Votre modèle est-il soumis à un overfitting ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB,BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: Green; padding: 7px;\" >\n",
    "\n",
    "### 4.2.Optimisation du modèle retenu\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: Green; padding: 7px;\" >\n",
    "\n",
    "### 4.3.Sauvegarde du modèle entrainé\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du meilleur modele\n",
    "now = datetime.now() # current date and time\n",
    "date_time = now.strftime(\"%Y-%m-%d-%H_%M_%S\")\n",
    "model_save_file_name = 'ema_lannuontimes_saved_model_' + date_time + '.joblib'\n",
    "# Attention, il faudra mettre à jour les colonnes correspondantes dans le premier if en cas de modification du model\n",
    "dump(model_to_save, curent_path+model_save_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: Green; padding: 7px;\" >\n",
    "\n",
    "### 4.4.Prédiction\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_file_name = 'ema_lannuontimes_saved_model_' + date_time + '.joblib'\n",
    "model_save_path = curent_path+ model_save_file_name\n",
    "\n",
    "if path.exists(model_save_path) and path.isfile(model_save_path):\n",
    "    # Chargement du modèle pré-entrainer\n",
    "    better_model = load(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; background-color: Blue; padding: 15px;\" >\n",
    "\n",
    "## ANNEXE\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
